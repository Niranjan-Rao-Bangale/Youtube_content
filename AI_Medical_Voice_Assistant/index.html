<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <link
    href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
    rel="stylesheet"
  />
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background-color: #f8f9fa;
      color: #333;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
      min-height: 100vh;
      margin: 0;
      padding: 20px;
    }
    h1 {
      color: #1976d2;
      margin-bottom: 20px;
    }
    /* Container must be relative so ring can be absolutely placed */
    #avatar-container {
      width: 200px;
      height: 200px;
      margin: 20px auto;
      border-radius: 50%;
      overflow: hidden;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);
      position: relative;
    }
    /* The rotating ring overlay */
    .rotating-ring {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: 5px solid #1976d2; /* The color & thickness of your ring */
      border-radius: 50%;
      box-sizing: border-box;
      pointer-events: none;      /* So it doesn't block clicks */
      animation: spin 2s linear infinite;
      opacity: 0;               /* Hidden by default */
      transition: opacity 0.3s;
      z-index: 2;
    }
    /* Show the ring only when container has "speaking" class */
    #avatar-container.speaking .rotating-ring {
      opacity: 1;
    }

    /* Keyframes for the rotation */
    @keyframes spin {
      0%   { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }

    #avatar-video,
    #avatarImage {
      width: 100%;
      height: 100%;
      object-fit: cover;
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
    }
    #avatar-video {
      display: none;
    }
    .controls {
      margin-top: 20px;
    }
    .control-btn {
      font-size: 16px;
      padding: 12px 24px;
      margin: 10px;
      border: none;
      border-radius: 25px;
      background-color: #1976d2;
      color: white;
      cursor: pointer;
      transition: background-color 0.3s;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    .control-btn.end {
      background-color: #d32f2f;
    }
    .control-btn:hover {
      background-color: #1565c0;
    }
    #recorded-text,
    #response-box {
      width: 80%;
      max-width: 600px;
      margin: 20px auto;
      padding: 20px;
      border-radius: 8px;
      background-color: white;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      text-align: left;
    }
    #recorded-text span {
      font-weight: normal;
      color: #1976d2;
    }
    #response-box span {
      display: block;
      margin-top: 10px;
    }
    #response-box .loading {
      border: 4px solid #f3f3f3;
      border-top: 4px solid #3498db;
      border-radius: 50%;
      width: 20px;
      height: 20px;
      animation: spin 2s linear infinite;
      display: inline-block;
    }
  </style>
</head>

<body>
<h1>Siri, Health Consultant &#x2705;</h1>
<h4>Discuss your health issues for free suggestions and advice.</h4>
<h5>Just a chatbot, not a doctor â€” might be wrong!</h5>

<div id="avatar-container">
  <!-- This is our rotating ring overlay -->
  <div class="rotating-ring"></div>

  <img id="avatarImage" src="avatar_neutral.png" alt="AI Avatar" />
  <video id="avatar-video" loop>
    <source src="avatar_talking.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</div>

<button id="startCall" class="control-btn">ðŸ“ž Start Call</button>
<button id="endCall" class="control-btn end" style="display: none;">ðŸ”´ End Call</button>

<p id="recorded-text">Recorded Text: <span></span></p>
<div id="response-box">AI Response: <span id="response"></span></div>

<script>
  let recognition;
  let isRecognizing = false;

  // Timers
  let stillWorkingTimer;   // 3-second "still working" fallback
  let anyMoreHelpTimer;    // 10-second "any more help" prompt
  let inactivityTimer;     // 2-minute inactivity to end call

  // On window load, show the neutral avatar
  window.onload = function() {
    document.getElementById("avatarImage").style.display = "block";
    document.getElementById("avatar-video").style.display = "none";
  };

  // Optional short filler audio
  function playAcknowledgmentAudio() {
    const acknowledgmentFiles = ['reflecting.mp3', 'hmm_processing.mp3', 'got_it.mp3'];
    const randomAckAudio = acknowledgmentFiles[Math.floor(Math.random() * acknowledgmentFiles.length)];
    return playAIResponse(randomAckAudio, "Mmm, let me reflect on it. Please give me a moment.");
  }

  // Main helper to animate avatar + play audio + show text
  function playAIResponse(audioUrl, text) {
    console.log("Playing AI response:", audioUrl, text);
    document.getElementById("response").innerText = text;

    const avatarContainer = document.getElementById("avatar-container");
    const avatarImage = document.getElementById("avatarImage");
    const avatarVideo = document.getElementById("avatar-video");

    // Show ring & talking video
    avatarContainer.classList.add("speaking");  // <-- Add the .speaking class
    avatarImage.style.display = "none";
    avatarVideo.style.display = "block";
    avatarVideo.currentTime = 0;
    avatarVideo.play();

    // Start audio
    const audio = new Audio(audioUrl);
    audio.play().catch(error => {
      console.error("Audio playback error:", error);
    });

    // Return a Promise that resolves when the audio ends
    return new Promise((resolve) => {
      audio.onended = () => {
        console.log("Audio ended:", audioUrl);
        // Stop ring & hide talking video
        avatarContainer.classList.remove("speaking"); // remove ring
        avatarVideo.pause();
        avatarVideo.style.display = "none";
        avatarImage.style.display = "block";
        resolve();
      };
      audio.onerror = (error) => {
        console.error("Audio playback error:", error);
        // Also remove ring so it doesn't get stuck
        avatarContainer.classList.remove("speaking");
        resolve(); // still resolve so the flow continues
      };
    });
  }

  // Start Call button
  async function startCall() {
    document.getElementById("startCall").style.display = "none";
    document.getElementById("endCall").style.display = "inline";

    // Tiny delay before greeting
    await new Promise((r) => setTimeout(r, 500));

    // Greet user
    await playAIResponse('initial_greeting.mp3',
      "Hello! I am Siri, AI Health Consultant. How can I help you today?"
    );

    // Now start recognition for multi-turn Q&A
    startRecording();
  }

  // End Call button
  function endCall() {
    document.getElementById("startCall").style.display = "inline";
    document.getElementById("endCall").style.display = "none";

    if (recognition && isRecognizing) {
      recognition.stop();
      isRecognizing = false;
    }

    // Clear all timers
    clearTimeout(stillWorkingTimer);
    clearTimeout(anyMoreHelpTimer);
    clearTimeout(inactivityTimer);

    document.getElementById("response").innerText = "Call ended.";
    document.getElementById("avatarImage").style.display = "block";
    document.getElementById("avatar-video").style.display = "none";

    // Also remove the 'speaking' class (if any)
    document.getElementById("avatar-container").classList.remove("speaking");
  }

  // Start speech recognition (continuous)
  async function startRecording() {
    if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      alert("Your browser does not support Speech Recognition. Try using Chrome or Edge.");
      return;
    }

    recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'en-US';
    recognition.continuous = true;
    recognition.interimResults = false;

    recognition.onstart = function() {
      isRecognizing = true;
      console.log("Recognition started...");
    };

    // When we get a final recognized utterance from user:
    recognition.onresult = async function (event) {
      // User spoke, so clear any old "anyMoreHelp" or "inactivity" timers
      clearTimeout(anyMoreHelpTimer);
      clearTimeout(inactivityTimer);

      // Grab recognized text
      const userText = event.results[0][0].transcript;
      console.log("User said:", userText);

      // Show in UI
      document.querySelector("#recorded-text span").innerText = userText;
      document.getElementById("response").innerHTML = '<span class="loading"></span>';

      // Possibly play a short acknowledgment BEFORE fetching
      await playAcknowledgmentAudio();

      // Start a "still working" fallback after 3 seconds if LLM is slow
      stillWorkingTimer = setTimeout(() => {
        const audioFiles = ['still_working.mp3', 'processing.mp3', 'hold_on.mp3'];
        const randomAudio = audioFiles[Math.floor(Math.random() * audioFiles.length)];
        const workingAudio = new Audio(randomAudio);
        workingAudio.play().catch(console.error);

        document.getElementById("response").innerText =
          "I'm still working on your request, please wait...";
      }, 3000);

      try {
        // Call your backend (FastAPI)
        const res = await fetch('http://127.0.0.1:8000/chat/', {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: userText })
        });

        document.getElementById("response").innerHTML = ''; // remove loading
        clearTimeout(stillWorkingTimer); // no longer needed

        if (!res.ok) {
          console.error("Error in API call", res.status);
          document.getElementById("response").innerText = "Error getting response!";
          return;
        }

        const data = await res.json();
        console.log("LLM data:", data);

        // Play the LLM's TTS answer
        await playAIResponse(data.audio_url, data.text);

        // --- Now we wait to see if user wants to ask more. ---
        // If user remains silent for 10 seconds, ask "any more help?"
        anyMoreHelpTimer = setTimeout(async () => {
          // If they're still silent after 10s, we do a courtesy ask:
          await playAIResponse("any_more_help.mp3", "Is there anything else I can help you with?");
          // Then set a final inactivity timer. If user doesn't speak for 2 min, end call
          inactivityTimer = setTimeout(() => {
            console.log("User has been inactive for 2 min; ending call.");
            endCall();
          }, 2 * 60 * 1000);
        }, 10 * 1000);

      } catch (error) {
        console.error("Fetch error:", error);
        document.getElementById("response").innerText = "Server error!";
      }
    };

    recognition.onerror = function (event) {
      if (event.error === "no-speech") {
        console.log("User was silent, no speech detected.");
        return;
      }
      console.error("Speech recognition error:", event.error);
      document.getElementById("response").innerText = "Speech recognition failed!";
      if (isRecognizing) {
        console.log("Restarting recognition after error.");
        recognition.start();
      }
    };

    recognition.onspeechend = function() {
      console.log("User paused speaking...");
      // We do not necessarily stop recognition. We let them keep talking if they want,
      // so the code for multi-turn remains active.
    };

    // Request mic permission and start recognition
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      recognition.start();
    } catch (error) {
      console.error("Speech recognition error:", error);
      document.getElementById("response").innerText = "Speech recognition failed!";
    }
  }

  // Wire up the buttons
  document.getElementById("startCall").addEventListener("click", startCall);
  document.getElementById("endCall").addEventListener("click", endCall);
</script>
</body>
</html>
