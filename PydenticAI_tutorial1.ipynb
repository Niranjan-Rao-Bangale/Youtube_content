{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=\"You are an AI assistant. Answer concisely: {input}\",\n",
        ")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = OpenAI(model=\"text-davinci-003\", api_key=\"your_openai_api_key\")\n",
        "\n",
        "# Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Use the chain to process input\n",
        "response = chain.run({\"input\": \"What is LangChain?\"})\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "x7G4CSqiIi6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "# Initialize the agent\n",
        "agent = Agent(\"gemini-1.5-flash\", system_prompt=\"Be concise.\")\n",
        "\n",
        "# Run the agent with a simple prompt\n",
        "response = agent.run_sync(\"What is PydanticAI?\")\n",
        "print(\"Response:\", response.data)"
      ],
      "metadata": {
        "id": "W22X-ATUIkWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2K9pDN06ekP-",
        "outputId": "7ce15d69-2439-47cd-e850-2df94dd9a99e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic-ai in /usr/local/lib/python3.10/dist-packages (0.0.15)\n",
            "Requirement already satisfied: pydantic-ai-slim==0.0.15 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.0.15)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.2.0)\n",
            "Requirement already satisfied: griffe>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.5.4)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.27.2)\n",
            "Requirement already satisfied: logfire-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.11.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.10.3)\n",
            "Requirement already satisfied: openai>=1.54.3 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.57.4)\n",
            "Requirement already satisfied: anthropic>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.42.0)\n",
            "Requirement already satisfied: groq>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.13.1)\n",
            "Requirement already satisfied: google-auth>=2.36.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.37.0)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.32.3)\n",
            "Requirement already satisfied: mistralai>=1.2.5 in /usr/local/lib/python3.10/dist-packages (from pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.2.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (4.9)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/dist-packages (from griffe>=1.3.2->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.4.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.2->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.2->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.2->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.2->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.14.0)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.0.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.3->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->pydantic-ai-slim==0.0.15->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic>=0.40.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->mistralai>=1.2.5->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai>=1.2.5->pydantic-ai-slim[anthropic,groq,mistral,openai,vertexai]==0.0.15->pydantic-ai) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hello world example for PydenticAi"
      ],
      "metadata": {
        "id": "m4VOigFSw1PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] = 'AIzaSyAl8X2o-QPkGIerC9X2GM2uS3P7nLiYHco'\n",
        "\n",
        "from pydantic_ai import Agent\n",
        "\n",
        "\n",
        "async def main():\n",
        "  agent = Agent('gemini-1.5-flash', system_prompt='Be concise.')\n",
        "  result = await agent.run('Why choose PydanticAI?')  # Use `run` for async execution\n",
        "  print(f\"result: {result}\")\n",
        "  response_text = result.data.strip()  # Extract and clean up the response\n",
        "  print(f\"Look at LLM response: {response_text}\")\n",
        "\n",
        "# Run the async function\n",
        "await main()\n",
        "\n"
      ],
      "metadata": {
        "id": "qyHNo128emtw",
        "outputId": "50e2d114-2519-4655-8b52-b7923d8a2d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: RunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content='Be concise.', part_kind='system-prompt'), UserPromptPart(content='Why choose PydanticAI?', timestamp=datetime.datetime(2024, 12, 29, 18, 7, 21, 639595, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"PydanticAI offers streamlined data validation and AI model integration within a Python application.  It combines Pydantic's strong typing with AI capabilities for enhanced data management.\\n\", part_kind='text')], timestamp=datetime.datetime(2024, 12, 29, 18, 7, 22, 337136, tzinfo=datetime.timezone.utc), kind='response')], _new_message_index=0, data=\"PydanticAI offers streamlined data validation and AI model integration within a Python application.  It combines Pydantic's strong typing with AI capabilities for enhanced data management.\\n\", _usage=Usage(requests=1, request_tokens=12, response_tokens=36, total_tokens=48, details=None))\n",
            "Look at LLM response: PydanticAI offers streamlined data validation and AI model integration within a Python application.  It combines Pydantic's strong typing with AI capabilities for enhanced data management.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the async main function\n",
        "async def main(agent, prompt):\n",
        "    result = await agent.run(prompt)  # Use the `run` method for async execution\n",
        "    print(f\"Complete result: {result}\")\n",
        "    print(f\"Look at LLM required response: {result.data.strip()}\")\n",
        "\n",
        "# Create the Agent instance\n",
        "agent = Agent('gemini-1.5-flash', system_prompt='Be concise.')\n",
        "prompt = 'Why choose PydanticAI?'\n",
        "await main(agent, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BitfRI51thl-",
        "outputId": "9a6902a8-ca18-413b-eec7-c224739d7384"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: RunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content='Be concise.', part_kind='system-prompt'), UserPromptPart(content='Why choose PydanticAI?', timestamp=datetime.datetime(2024, 12, 29, 13, 59, 57, 648306, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"PydanticAI combines Pydantic's data validation with AI model integration, streamlining data handling and AI workflow in Python.\\n\", part_kind='text')], timestamp=datetime.datetime(2024, 12, 29, 13, 59, 58, 244355, tzinfo=datetime.timezone.utc), kind='response')], _new_message_index=0, data=\"PydanticAI combines Pydantic's data validation with AI model integration, streamlining data handling and AI workflow in Python.\\n\", _usage=Usage(requests=1, request_tokens=12, response_tokens=27, total_tokens=39, details=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# Type safe\n",
        "# Define the WeatherInfo model\n",
        "class WeatherInfo(BaseModel):\n",
        "    temperature: float\n",
        "    condition: str\n",
        "\n",
        "# Mock implementation for fetching weather data\n",
        "async def weather_api_get(city: str) -> float:\n",
        "    # Simulate fetching data from an API\n",
        "    if city.lower() == \"colorado, highlands ranch\":\n",
        "        return {\"temperature\": 72.5, \"condition\": \"Sunny\"}\n",
        "    return {\"temperature\": 68.0, \"condition\": \"Cloudy\"}\n",
        "\n",
        "# # You can get Actual data using this below method by uncommenting and running it by replacing your_openweather_api_key with actual key.\n",
        "# # Set the API key for OpenWeatherMap\n",
        "# os.environ['OPENWEATHER_API_KEY'] = 'your_openweather_api_key'\n",
        "# # Fetch real-time weather data\n",
        "# async def weather_api_get(city: str) -> dict:\n",
        "#     \"\"\"Fetch current weather data for a given city using OpenWeatherMap API.\"\"\"\n",
        "#     api_key = os.getenv('OPENWEATHER_API_KEY')\n",
        "#     if not api_key:\n",
        "#         raise ValueError(\"OPENWEATHER_API_KEY environment variable is not set.\")\n",
        "\n",
        "#     # Build the OpenWeatherMap API URL\n",
        "#     base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "#     params = {\n",
        "#         \"q\": city,\n",
        "#         \"appid\": api_key,\n",
        "#         \"units\": \"imperial\",  # For Fahrenheit. Use \"metric\" for Celsius.\n",
        "#     }\n",
        "\n",
        "#     async with httpx.AsyncClient() as client:\n",
        "#         response = await client.get(base_url, params=params)\n",
        "#         if response.status_code != 200:\n",
        "#             raise ValueError(f\"Error fetching weather data: {response.json()}\")\n",
        "\n",
        "#         data = response.json()\n",
        "#         return {\n",
        "#             \"temperature\": data[\"main\"][\"temp\"],\n",
        "#             \"condition\": data[\"weather\"][0][\"description\"].capitalize(),\n",
        "#         }\n",
        "\n",
        "# Define the tool\n",
        "@Agent.tool\n",
        "async def get_temperature(city: str) -> WeatherInfo:\n",
        "    \"\"\"Fetch current temperature and condition for a city.\"\"\"\n",
        "    data = await weather_api_get(city)\n",
        "    return WeatherInfo(**data)\n",
        "\n",
        "# Define the async main function\n",
        "async def main(agent, prompt):\n",
        "    result = await agent.run(prompt)\n",
        "    print(f\"Structured Weather Data: {result}\")\n",
        "\n",
        "# Create the weather agent\n",
        "weather_agent = Agent('gemini-1.5-flash', result_type=WeatherInfo)\n",
        "\n",
        "# Define the prompt\n",
        "prompt = 'How is the weather today in Colorado, Highlands Ranch?'\n",
        "\n",
        "# Run the async main function with the agent and prompt\n",
        "await main(weather_agent, prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4xzMXE_rGE4",
        "outputId": "20727974-78ee-4fd5-9157-fb61d5991ee6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structured Weather Data: RunResult(_all_messages=[ModelRequest(parts=[UserPromptPart(content='How is the weather today in Colorado, Highlands Ranch?', timestamp=datetime.datetime(2024, 12, 29, 18, 24, 47, 91979, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='final_result', args=ArgsDict(args_dict={'condition': 'sunny', 'temperature': 25.5}), tool_call_id=None, part_kind='tool-call')], timestamp=datetime.datetime(2024, 12, 29, 18, 24, 47, 798466, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id=None, timestamp=datetime.datetime(2024, 12, 29, 18, 24, 47, 799916, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request')], _new_message_index=0, data=WeatherInfo(temperature=25.5, condition='sunny'), _usage=Usage(requests=1, request_tokens=62, response_tokens=6, total_tokens=68, details=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk3RliQxElY1",
        "outputId": "5f6953d6-d127-4bd2-a251-e4bc64a2e4dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "# Allow nested event loops for Colab\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "t-BiMHeLxoof"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mock database connection\n",
        "class DatabaseConn:\n",
        "    \"\"\"This is a mock database for example purposes.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    async def customer_name(cls, *, id: int) -> str | None:\n",
        "        if id == 123:\n",
        "            return 'John Doe'\n",
        "        return None\n",
        "\n",
        "    @classmethod\n",
        "    async def customer_balance(cls, *, id: int, include_pending: bool) -> float:\n",
        "        if id == 123:\n",
        "            return 1234.56\n",
        "        raise ValueError('Customer not found')"
      ],
      "metadata": {
        "id": "TdgkOuGcEFpm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from pydantic import Field\n",
        "# Dependencies to be injected into the agent\n",
        "@dataclass\n",
        "class SupportDependencies:\n",
        "    customer_id: int\n",
        "    db: DatabaseConn\n",
        "\n",
        "# Define the response model\n",
        "class SupportResult(BaseModel):\n",
        "    support_advice: str = Field(description='Advice returned to the customer')\n",
        "    block_card: bool = Field(description=\"Whether to block the customer's card\")\n",
        "    risk: int = Field(description='Risk level of query', ge=0, le=10)"
      ],
      "metadata": {
        "id": "_LeXkluXER3_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the agent\n",
        "support_agent = Agent(\n",
        "    model='gemini-1.5-flash',\n",
        "    deps_type=SupportDependencies,\n",
        "    result_type=SupportResult,\n",
        "    system_prompt=(\n",
        "        'You are a support agent in our bank. Provide the customer with support and assess the risk level of their query. '\n",
        "        \"Address the customer by their name.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "qfI7BQArEV3Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import RunContext\n",
        "# System prompt to personalize customer response\n",
        "@support_agent.system_prompt\n",
        "async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n",
        "    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n",
        "    return f\"The customer's name is {customer_name}.\""
      ],
      "metadata": {
        "id": "0rvE9ZoyEYHh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tool to fetch account balance\n",
        "@support_agent.tool\n",
        "async def customer_balance(\n",
        "    ctx: RunContext[SupportDependencies], include_pending: bool\n",
        ") -> str:\n",
        "    \"\"\"Returns the customer's current account balance.\"\"\"\n",
        "    balance = await ctx.deps.db.customer_balance(\n",
        "        id=ctx.deps.customer_id,\n",
        "        include_pending=include_pending,\n",
        "    )\n",
        "    return f'${balance:.2f}'"
      ],
      "metadata": {
        "id": "3zsWT7BdEa5I"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Async function to run the agent\n",
        "async def main(agent, prompt):\n",
        "    # Simulate dependencies\n",
        "    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n",
        "    # Run the agent\n",
        "    result = await agent.run(prompt, deps=deps)\n",
        "    print(f\"Structured Support Response: {result.data}\")\n",
        "\n",
        "# Define the customer prompt\n",
        "prompt = 'What is my balance?'\n",
        "\n",
        "# Run the main function\n",
        "await main(support_agent, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTCGZz0XFUuY",
        "outputId": "219534ec-d4fe-404f-c2ab-0abc6bfc91f4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structured Support Response: support_advice='Your current balance is $1234.56.  Is there anything else I can help you with?' block_card=False risk=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the customer prompt\n",
        "prompt = 'I just lost my card!'\n",
        "\n",
        "# Run the main function\n",
        "await main(support_agent, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI6paXUBFb7K",
        "outputId": "b951bb3d-62d2-4c3c-9a60-ec6eda67c3bb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structured Support Response: support_advice=\"John Doe, I'm sorry to hear that you lost your card. I have blocked your card to prevent any unauthorized transactions. Please contact us as soon as possible to request a new one.\" block_card=True risk=2\n"
          ]
        }
      ]
    }
  ]
}